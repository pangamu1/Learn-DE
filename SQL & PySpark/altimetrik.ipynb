{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9a7e4e",
   "metadata": {},
   "source": [
    "# **Altimetrik Interview Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c1e04",
   "metadata": {},
   "source": [
    "### **Question 1:**\n",
    "**Schema:** `order_id`, `order_no`, `updated_at`, `amount`\\\n",
    "\\\n",
    "(i) Identify duplicate `order_no` values.\\\n",
    "(ii) For each duplicated `order_no`, return only the row with the most recent record.\\\n",
    "(iii) Exclude order numbers that appear only once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ec4aa3",
   "metadata": {},
   "source": [
    "### **PySpark Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463d4ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856614af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/06 18:44:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created and the version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName(\"OrderAnalysis\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reduce logs\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark Session created and the version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f230b43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------------+------+\n",
      "|order_id|order_no|         updated_at|amount|\n",
      "+--------+--------+-------------------+------+\n",
      "|       1|    A100|2024-05-01 10:00:00| 250.0|\n",
      "|       2|    A100|2024-05-01 10:00:00| 260.0|\n",
      "|       3|    A100|2024-05-01 10:00:00| 265.0|\n",
      "|       4|    B200|2024-05-01 10:00:00| 300.0|\n",
      "|       5|    C300|2024-05-01 10:00:00| 150.0|\n",
      "|       6|    C300|2024-05-01 10:00:00| 155.0|\n",
      "|       7|    D400|2024-05-01 10:00:00| 180.0|\n",
      "|       8|    D400|2024-05-01 10:00:00| 275.0|\n",
      "|       9|    E600|2024-05-01 10:00:00| 350.0|\n",
      "|      10|    E700|2024-05-01 10:00:00| 175.0|\n",
      "+--------+--------+-------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders_data = [\n",
    "    Row(order_id=1, order_no=\"A100\", updated_at=\"2024-05-01 10:00:00\", amount=250.00),\n",
    "    Row(order_id=2, order_no=\"A100\", updated_at=\"2024-05-01 10:00:00\", amount=260.00),\n",
    "    Row(order_id=3, order_no=\"A100\", updated_at=\"2024-05-01 10:00:00\", amount=265.00),\n",
    "    Row(order_id=4, order_no=\"B200\", updated_at=\"2024-05-01 10:00:00\", amount=300.00),\n",
    "    Row(order_id=5, order_no=\"C300\", updated_at=\"2024-05-01 10:00:00\", amount=150.00),\n",
    "    Row(order_id=6, order_no=\"C300\", updated_at=\"2024-05-01 10:00:00\", amount=155.00),\n",
    "    Row(order_id=7, order_no=\"D400\", updated_at=\"2024-05-01 10:00:00\", amount=180.00),\n",
    "    Row(order_id=8, order_no=\"D400\", updated_at=\"2024-05-01 10:00:00\", amount=275.00),\n",
    "    Row(order_id=9, order_no=\"E600\", updated_at=\"2024-05-01 10:00:00\", amount=350.00),\n",
    "    Row(order_id=10, order_no=\"E700\", updated_at=\"2024-05-01 10:00:00\", amount=175.00)\n",
    "    ]\n",
    "\n",
    "df = spark.createDataFrame(orders_data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de8e0227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|order_no|order_count|\n",
      "+--------+-----------+\n",
      "|    A100|          3|\n",
      "|    C300|          2|\n",
      "|    D400|          2|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dupe_df = df.groupBy(col('order_no')).agg(count(col('order_no')).alias('order_count'))\\\n",
    "    .filter(col('order_count') > 1)\n",
    "dupe_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc348600",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df.join(dupe_df,'order_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "613ebf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('order_no','updated_at').orderBy(col('order_id').desc())\n",
    "# Partitioning by these two columns will work only for this case since the timestamps are the same,\n",
    "# IRL scenaios, partiton by order_no and order it by timestamp. \n",
    "filter_df = joined_df.select('*',row_number().over(window_spec).alias(\"rank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94c26621",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = filter_df.select(col('order_id'),col('order_no'),col('updated_at'),col('amount')).filter(col('rank') == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0a57f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------------+------+\n",
      "|order_id|order_no|         updated_at|amount|\n",
      "+--------+--------+-------------------+------+\n",
      "|       3|    A100|2024-05-01 10:00:00| 265.0|\n",
      "|       6|    C300|2024-05-01 10:00:00| 155.0|\n",
      "|       8|    D400|2024-05-01 10:00:00| 275.0|\n",
      "+--------+--------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb39125f",
   "metadata": {},
   "source": [
    "### **SQL Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b1f02",
   "metadata": {},
   "source": [
    "```sql\n",
    "WITH filtered AS(\n",
    "\tSELECT \n",
    "\t\torder_no,\n",
    "        COUNT(*) total_orders\n",
    "\tFROM \n",
    "\t\torders\n",
    "\tGROUP BY 1\n",
    "    HAVING COUNT(*) > 1\n",
    "),\n",
    "dupe AS(\n",
    "\tSELECT\n",
    "\t\t*,\n",
    "\t\tROW_NUMBER() OVER(PARTITION BY order_no ORDER BY updated_at desc) rnk\n",
    "\tFROM\n",
    "\t\torders\n",
    "\tJOIN \n",
    "\t\tfiltered \n",
    "\tUSING(order_no)\n",
    ")\n",
    "SELECT \n",
    "\torder_id,\n",
    "    order_no,\n",
    "    updated_at,\n",
    "    amount\n",
    "FROM\n",
    "\tdupe\n",
    "WHERE \n",
    "\trnk = 1;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb960e6",
   "metadata": {},
   "source": [
    "### **Question 2:**\n",
    "**Schema:** `customer`, `date`, `amount`\\\n",
    "\\\n",
    "For each customer and each date, label whether transaction amount is: \\\n",
    "(i)Higher than previous \\\n",
    "(ii)Lower than previous \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0571c6a2",
   "metadata": {},
   "source": [
    "### **PySpark Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af1b3f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|customer|      date|amount|\n",
      "+--------+----------+------+\n",
      "|       A|2024-01-01| 100.0|\n",
      "|       A|2024-01-03| 150.0|\n",
      "|       A|2024-01-05| 120.0|\n",
      "|       A|2024-01-07| 120.0|\n",
      "|       A|2024-01-10| 200.0|\n",
      "|       B|2024-01-01| 300.0|\n",
      "|       B|2024-01-05| 300.0|\n",
      "|       B|2024-01-08| 150.0|\n",
      "|       B|2024-01-15| 500.0|\n",
      "+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_data = [\n",
    "    Row(customer='A', date=\"2024-01-01\", amount=100.00),\n",
    "    Row(customer='A', date=\"2024-01-03\", amount=150.00),\n",
    "    Row(customer='A', date=\"2024-01-05\", amount=120.00),\n",
    "    Row(customer='A', date=\"2024-01-07\", amount=120.00),\n",
    "    Row(customer='A', date=\"2024-01-10\", amount=200.00),\n",
    "    Row(customer='B', date=\"2024-01-01\", amount=300.00),\n",
    "    Row(customer='B', date=\"2024-01-05\", amount=300.00),\n",
    "    Row(customer='B', date=\"2024-01-08\", amount=150.00),\n",
    "    Row(customer='B', date=\"2024-01-15\", amount=500.00)\n",
    "]\n",
    "\n",
    "c_df = spark.createDataFrame(customer_data)\n",
    "c_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "063a3e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+--------------------+\n",
      "|customer|      date|amount|     amount_category|\n",
      "+--------+----------+------+--------------------+\n",
      "|       A|2024-01-01| 100.0|              ------|\n",
      "|       A|2024-01-03| 150.0|Higher than previous|\n",
      "|       A|2024-01-05| 120.0| Lower than previous|\n",
      "|       A|2024-01-07| 120.0|Same Amount as Pr...|\n",
      "|       A|2024-01-10| 200.0|Higher than previous|\n",
      "|       B|2024-01-01| 300.0|              ------|\n",
      "|       B|2024-01-05| 300.0|Same Amount as Pr...|\n",
      "|       B|2024-01-08| 150.0| Lower than previous|\n",
      "|       B|2024-01-15| 500.0|Higher than previous|\n",
      "+--------+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "win_spec = Window.partitionBy(col('customer')).orderBy(col('date'))\n",
    "prev_amount = lag('amount').over(win_spec)\n",
    "cat_df = c_df.withColumn('amount_category',\\\n",
    "                          when(col('amount')> prev_amount, 'Higher than previous').\\\n",
    "                          when(col('amount')< prev_amount, 'Lower than previous').\\\n",
    "                          when(isnull(prev_amount),'------').\\\n",
    "                          otherwise('Same Amount as Previous'))\n",
    "cat_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4815194",
   "metadata": {},
   "source": [
    "### **SQL Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff183c",
   "metadata": {},
   "source": [
    "```sql\n",
    "WITH cat AS(\n",
    "\tSELECT\n",
    "\t\t*,\n",
    "\t\tLAG(amount) OVER(PARTITION BY customer ORDER BY date) previous \n",
    "\tFROM\n",
    "\t\tcustomers \n",
    ")\n",
    "SELECT \n",
    "\tcustomer,\n",
    "    date,\n",
    "    amount,\n",
    "    CASE \n",
    "\t\tWHEN amount > previous THEN 'Higher than Previous'\n",
    "\t\tWHEN amount < previous THEN 'Lower than Previous'\n",
    "\t\tWHEN previous IS NULL THEN '------'\n",
    "\t\tELSE 'Same as Previous'\n",
    "\tEND AS amount_category\n",
    "FROM\n",
    "\tcat;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ac8644",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
